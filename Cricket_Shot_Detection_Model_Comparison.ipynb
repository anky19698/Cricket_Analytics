{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPMlJCFynaNHhqnuS5JUjx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anky19698/Cricket_Analytics/blob/main/Cricket_Shot_Detection_Model_Comparison.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detectron2 Models: Cricket Shot Detection\n",
        "### COCO Person Keypoint Detection Baselines with Keypoint R-CNN"
      ],
      "metadata": {
        "id": "vmLHiXKRsj8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) R50-FPN"
      ],
      "metadata": {
        "id": "7T8TmzHntFWV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlFnJ9DfsV11",
        "outputId": "a5590b95-2252-4c92-ea73-b495bf268588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1859 1859\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Importing Collected Keypoints for Cricket Shot Dataset from R50-FPN\n",
        "\n",
        "keypoints = []\n",
        "labels = []\n",
        "file_path = 'img_data_R50.csv'\n",
        "data = np.genfromtxt(file_path, delimiter=',')\n",
        "# print(data)\n",
        "\n",
        "for p in data:\n",
        "    keypoints.append(np.array(p))\n",
        "\n",
        "labels_path = 'img_data_R50_labels.csv'\n",
        "\n",
        "label_data = open(labels_path)\n",
        "\n",
        "for label in label_data.readlines():\n",
        "    labels.append(label.split('\\n')[0])\n",
        "\n",
        "\n",
        "print(len(keypoints), len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# for normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# define normalizer\n",
        "scaler= StandardScaler()\n",
        "# normalize keypoints\n",
        "keypoints = scaler.fit_transform(keypoints)\n",
        "# convert to an array\n",
        "keypoints = np.array(keypoints)"
      ],
      "metadata": {
        "id": "Mi7iiq07u-yQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the target categories into numbers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y=le.fit_transform(labels)"
      ],
      "metadata": {
        "id": "PDQgGoFLuu8N"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for creating training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# split keypoints and labels in 80:20\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(keypoints, y, test_size=0.2, stratify=labels,\n",
        "                                        random_state=120)"
      ],
      "metadata": {
        "id": "2ImXLs4BuuzL"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the keypoints and target value to tensor\n",
        "import torch\n",
        "x_tr = torch.Tensor(x_tr)\n",
        "x_val = torch.Tensor(x_val)\n",
        "y_tr = torch.Tensor(y_tr)\n",
        "y_tr = y_tr.type(torch.long)\n",
        "y_val = torch.Tensor(y_val)\n",
        "y_val = y_val.type(torch.long)"
      ],
      "metadata": {
        "id": "V6YuG-t-vFfE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of training and validation set\n",
        "(x_tr.shape, y_tr.shape), (x_val.shape, y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDcUnyd3vFN7",
        "outputId": "4f9d8500-8b08-43e0-e5d8-c382d2941756"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((torch.Size([1487, 34]), torch.Size([1487])),\n",
              " (torch.Size([372, 34]), torch.Size([372])))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries for defining the architecture of model\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Adam\n",
        "from torch.nn import Linear, ReLU, Sequential, Softmax, CrossEntropyLoss\n",
        "# defining the model architecture\n",
        "model = Sequential(Linear(34, 64),\n",
        "                   ReLU(),\n",
        "                   Linear(64, 4),\n",
        "                   Softmax()\n",
        "                   )"
      ],
      "metadata": {
        "id": "00Nr1I7RvLik"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define optimizer and loss function\n",
        "optimizer = Adam(model.parameters(), lr=0.01)\n",
        "criterion = CrossEntropyLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()"
      ],
      "metadata": {
        "id": "jniu8WqzvQb7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    # getting the training set\n",
        "    x_train, y_train = Variable(x_tr), Variable(y_tr)\n",
        "    # getting the validation set\n",
        "    x_valid, y_valid = Variable(x_val), Variable(y_val)\n",
        "    # converting the data into GPU format\n",
        "    if torch.cuda.is_available():\n",
        "        x_train = x_train.cuda()\n",
        "        y_train = y_train.cuda()\n",
        "        x_valid = x_valid.cuda()\n",
        "        y_valid = y_valid.cuda()\n",
        "    # clearing the Gradients of the model parameters\n",
        "    optimizer.zero_grad()\n",
        "    # prediction for training and validation set\n",
        "    output_train = model(x_train)\n",
        "    output_val = model(x_valid)\n",
        "    # computing the training and validation loss\n",
        "    loss_train = criterion(output_train, y_train)\n",
        "    loss_val = criterion(output_val, y_valid)\n",
        "    # computing the updated weights of all the model parameters\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    if epoch%1000 == 0:\n",
        "        # printing the validation loss\n",
        "        print('Epoch : ',epoch+1, 't', 'loss :', loss_val.item())"
      ],
      "metadata": {
        "id": "ljM9zeSIvQPX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the number of epochs\n",
        "n_epochs = 5000\n",
        "# training the model\n",
        "for epoch in range(n_epochs):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0MCUorSvXmS",
        "outputId": "9b12756a-a6cb-41ed-8da7-fbaca7a951f7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch :  1 t loss : 1.3774304389953613\n",
            "Epoch :  1001 t loss : 0.9194391965866089\n",
            "Epoch :  2001 t loss : 0.9173290133476257\n",
            "Epoch :  3001 t loss : 0.9138457775115967\n",
            "Epoch :  4001 t loss : 0.9145391583442688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to check the model performance\n",
        "from sklearn.metrics import accuracy_score\n",
        "# get validation accuracy\n",
        "x, y = Variable(x_val), Variable(y_val)\n",
        "if torch.cuda.is_available():\n",
        "  x_val = x.cuda()\n",
        "  y_val = y.cuda()\n",
        "pred = model(x_val)\n",
        "final_pred = np.argmax(pred.cpu().data.numpy(), axis=1)\n",
        "\n",
        "print(\"Model Test Accuracy:\", round(accuracy_score(y_val.cpu(), final_pred)*100, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCR2q-ogvsmo",
        "outputId": "f6f2487d-de8b-474e-d5f3-e9cb414c03b1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Test Accuracy: 82.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) R101-FPN"
      ],
      "metadata": {
        "id": "MpdSOMeutiNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Collected Keypoints for Cricket Shot Dataset from R101-FPN\n",
        "\n",
        "keypoints = []\n",
        "labels = []\n",
        "file_path = 'img_data_R101.csv'\n",
        "data = np.genfromtxt(file_path, delimiter=',')\n",
        "# print(data)\n",
        "\n",
        "for p in data:\n",
        "    keypoints.append(np.array(p))\n",
        "\n",
        "labels_path = 'img_data_R101_labels.csv'\n",
        "\n",
        "label_data = open(labels_path)\n",
        "\n",
        "for label in label_data.readlines():\n",
        "    labels.append(label.split('\\n')[0])\n",
        "\n",
        "\n",
        "print(len(keypoints), len(labels))"
      ],
      "metadata": {
        "id": "rVWcOMFLtrUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752d3cbf-9a8b-412a-e7b2-e43070e1dbf6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1240 1240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# for normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# define normalizer\n",
        "scaler= StandardScaler()\n",
        "# normalize keypoints\n",
        "keypoints = scaler.fit_transform(keypoints)\n",
        "# convert to an array\n",
        "keypoints = np.array(keypoints)"
      ],
      "metadata": {
        "id": "elQmhltTwu1h"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the target categories into numbers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y=le.fit_transform(labels)"
      ],
      "metadata": {
        "id": "RhyiBw7lwv-q"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for creating training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# split keypoints and labels in 80:20\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(keypoints, y, test_size=0.2, stratify=labels,\n",
        "                                        random_state=120)"
      ],
      "metadata": {
        "id": "N9L1aX6-wv8l"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the keypoints and target value to tensor\n",
        "import torch\n",
        "x_tr = torch.Tensor(x_tr)\n",
        "x_val = torch.Tensor(x_val)\n",
        "y_tr = torch.Tensor(y_tr)\n",
        "y_tr = y_tr.type(torch.long)\n",
        "y_val = torch.Tensor(y_val)\n",
        "y_val = y_val.type(torch.long)"
      ],
      "metadata": {
        "id": "NIDk8xmdwv6Z"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of training and validation set\n",
        "(x_tr.shape, y_tr.shape), (x_val.shape, y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YYd18PAwv4E",
        "outputId": "e62b221f-1f23-40ac-9d75-02ee61b23f8e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((torch.Size([992, 34]), torch.Size([992])),\n",
              " (torch.Size([248, 34]), torch.Size([248])))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries for defining the architecture of model\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Adam\n",
        "from torch.nn import Linear, ReLU, Sequential, Softmax, CrossEntropyLoss\n",
        "# defining the model architecture\n",
        "model = Sequential(Linear(34, 64),\n",
        "                   ReLU(),\n",
        "                   Linear(64, 4),\n",
        "                   Softmax()\n",
        "                   )"
      ],
      "metadata": {
        "id": "S09X945Rw6Oq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define optimizer and loss function\n",
        "optimizer = Adam(model.parameters(), lr=0.01)\n",
        "criterion = CrossEntropyLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()"
      ],
      "metadata": {
        "id": "L6h6b-Ztwv2N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    # getting the training set\n",
        "    x_train, y_train = Variable(x_tr), Variable(y_tr)\n",
        "    # getting the validation set\n",
        "    x_valid, y_valid = Variable(x_val), Variable(y_val)\n",
        "    # converting the data into GPU format\n",
        "    if torch.cuda.is_available():\n",
        "        x_train = x_train.cuda()\n",
        "        y_train = y_train.cuda()\n",
        "        x_valid = x_valid.cuda()\n",
        "        y_valid = y_valid.cuda()\n",
        "    # clearing the Gradients of the model parameters\n",
        "    optimizer.zero_grad()\n",
        "    # prediction for training and validation set\n",
        "    output_train = model(x_train)\n",
        "    output_val = model(x_valid)\n",
        "    # computing the training and validation loss\n",
        "    loss_train = criterion(output_train, y_train)\n",
        "    loss_val = criterion(output_val, y_valid)\n",
        "    # computing the updated weights of all the model parameters\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    if epoch%1000 == 0:\n",
        "        # printing the validation loss\n",
        "        print('Epoch : ',epoch+1, 't', 'loss :', loss_val.item())"
      ],
      "metadata": {
        "id": "qQFGfh0pw_om"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the number of epochs\n",
        "n_epochs = 5000\n",
        "# training the model\n",
        "for epoch in range(n_epochs):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec3-IF2nw_lW",
        "outputId": "5bb921a5-57bd-4cf5-830b-c8ef7d35f38e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch :  1 t loss : 1.3776861429214478\n",
            "Epoch :  1001 t loss : 0.909116268157959\n",
            "Epoch :  2001 t loss : 0.9032103419303894\n",
            "Epoch :  3001 t loss : 0.9021798372268677\n",
            "Epoch :  4001 t loss : 0.900928795337677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to check the model performance\n",
        "from sklearn.metrics import accuracy_score\n",
        "# get validation accuracy\n",
        "x, y = Variable(x_val), Variable(y_val)\n",
        "if torch.cuda.is_available():\n",
        "  x_val = x.cuda()\n",
        "  y_val = y.cuda()\n",
        "pred = model(x_val)\n",
        "final_pred = np.argmax(pred.cpu().data.numpy(), axis=1)\n",
        "\n",
        "print(\"Model Test Accuracy:\", round(accuracy_score(y_val.cpu(), final_pred)*100, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGsFEvZ_xTp4",
        "outputId": "caa5b8dc-ba80-4450-f654-c6b1d9935db5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Test Accuracy: 83.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) X101-FPN"
      ],
      "metadata": {
        "id": "qC5WqI_StlGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Collected Keypoints for Cricket Shot Dataset from X101-FPN\n",
        "\n",
        "keypoints = []\n",
        "labels = []\n",
        "file_path = 'img_data_X101.csv'\n",
        "data = np.genfromtxt(file_path, delimiter=',')\n",
        "# print(data)\n",
        "\n",
        "for p in data:\n",
        "    keypoints.append(np.array(p))\n",
        "\n",
        "labels_path = 'img_data_X101_labels.csv'\n",
        "\n",
        "label_data = open(labels_path)\n",
        "\n",
        "for label in label_data.readlines():\n",
        "    labels.append(label.split('\\n')[0])\n",
        "\n",
        "\n",
        "print(len(keypoints), len(labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J68NhEJtsSI",
        "outputId": "91d2b385-fb5c-4591-8b40-55b063fa1d74"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "620 620\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# for normalization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# define normalizer\n",
        "scaler= StandardScaler()\n",
        "# normalize keypoints\n",
        "keypoints = scaler.fit_transform(keypoints)\n",
        "# convert to an array\n",
        "keypoints = np.array(keypoints)"
      ],
      "metadata": {
        "id": "Bdv2K3lPz7Kn"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the target categories into numbers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y=le.fit_transform(labels)"
      ],
      "metadata": {
        "id": "6OQU6kZl0A1f"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for creating training and validation sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "# split keypoints and labels in 80:20\n",
        "x_tr, x_val, y_tr, y_val = train_test_split(keypoints, y, test_size=0.2, stratify=labels,\n",
        "                                        random_state=120)"
      ],
      "metadata": {
        "id": "qCWCQ45E0C53"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the keypoints and target value to tensor\n",
        "import torch\n",
        "x_tr = torch.Tensor(x_tr)\n",
        "x_val = torch.Tensor(x_val)\n",
        "y_tr = torch.Tensor(y_tr)\n",
        "y_tr = y_tr.type(torch.long)\n",
        "y_val = torch.Tensor(y_val)\n",
        "y_val = y_val.type(torch.long)"
      ],
      "metadata": {
        "id": "7DL9Obrm0E7r"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of training and validation set\n",
        "(x_tr.shape, y_tr.shape), (x_val.shape, y_val.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab1FNdPo0Gpf",
        "outputId": "56fc8392-155b-4993-a294-b22f6ee37b7a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((torch.Size([496, 34]), torch.Size([496])),\n",
              " (torch.Size([124, 34]), torch.Size([124])))"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries for defining the architecture of model\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Adam\n",
        "from torch.nn import Linear, ReLU, Sequential, Softmax, CrossEntropyLoss\n",
        "# defining the model architecture\n",
        "model = Sequential(Linear(34, 64),\n",
        "                   ReLU(),\n",
        "                   Linear(64, 4),\n",
        "                   Softmax()\n",
        "                   )"
      ],
      "metadata": {
        "id": "jEGr2FqK0JFt"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define optimizer and loss function\n",
        "optimizer = Adam(model.parameters(), lr=0.01)\n",
        "criterion = CrossEntropyLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()"
      ],
      "metadata": {
        "id": "LCa0P8yU0LP4"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    tr_loss = 0\n",
        "    # getting the training set\n",
        "    x_train, y_train = Variable(x_tr), Variable(y_tr)\n",
        "    # getting the validation set\n",
        "    x_valid, y_valid = Variable(x_val), Variable(y_val)\n",
        "    # converting the data into GPU format\n",
        "    if torch.cuda.is_available():\n",
        "        x_train = x_train.cuda()\n",
        "        y_train = y_train.cuda()\n",
        "        x_valid = x_valid.cuda()\n",
        "        y_valid = y_valid.cuda()\n",
        "    # clearing the Gradients of the model parameters\n",
        "    optimizer.zero_grad()\n",
        "    # prediction for training and validation set\n",
        "    output_train = model(x_train)\n",
        "    output_val = model(x_valid)\n",
        "    # computing the training and validation loss\n",
        "    loss_train = criterion(output_train, y_train)\n",
        "    loss_val = criterion(output_val, y_valid)\n",
        "    # computing the updated weights of all the model parameters\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "    if epoch%1000 == 0:\n",
        "        # printing the validation loss\n",
        "        print('Epoch : ',epoch+1, 't', 'loss :', loss_val.item())"
      ],
      "metadata": {
        "id": "ujN31aZJ0NDV"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the number of epochs\n",
        "n_epochs = 5000\n",
        "# training the model\n",
        "for epoch in range(n_epochs):\n",
        "    train(epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5U3VO0i0PBv",
        "outputId": "8021f1e5-bcff-4a22-a6ff-52d8859726eb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self._call_impl(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch :  1 t loss : 1.3933820724487305\n",
            "Epoch :  1001 t loss : 0.9697365760803223\n",
            "Epoch :  2001 t loss : 0.9555087089538574\n",
            "Epoch :  3001 t loss : 0.9556950330734253\n",
            "Epoch :  4001 t loss : 0.9569217562675476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to check the model performance\n",
        "from sklearn.metrics import accuracy_score\n",
        "# get validation accuracy\n",
        "x, y = Variable(x_val), Variable(y_val)\n",
        "if torch.cuda.is_available():\n",
        "  x_val = x.cuda()\n",
        "  y_val = y.cuda()\n",
        "pred = model(x_val)\n",
        "final_pred = np.argmax(pred.cpu().data.numpy(), axis=1)\n",
        "\n",
        "print(\"Model Test Accuracy:\", round(accuracy_score(y_val.cpu(), final_pred)*100, 2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHUyNKMS0RMk",
        "outputId": "5c6ab2d9-128d-47e6-c2a8-7b31a0b70708"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Test Accuracy: 79.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SEh29Cy5CZE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}